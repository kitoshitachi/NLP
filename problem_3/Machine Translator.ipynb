{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSuJ-9X_qk1b",
        "papermill": {
          "duration": 0.035121,
          "end_time": "2020-09-16T13:42:35.566107",
          "exception": false,
          "start_time": "2020-09-16T13:42:35.530986",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Necessary Imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-16T13:42:35.644453Z",
          "iopub.status.busy": "2020-09-16T13:42:35.643680Z",
          "iopub.status.idle": "2020-09-16T13:42:48.157700Z",
          "shell.execute_reply": "2020-09-16T13:42:48.157103Z"
        },
        "id": "S-Ycz13hbUbC",
        "papermill": {
          "duration": 12.557466,
          "end_time": "2020-09-16T13:42:48.157807",
          "exception": false,
          "start_time": "2020-09-16T13:42:35.600341",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# !pip install contractions\n",
        "# !pip install pyvi\n",
        "import torch\n",
        "! pip install torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from pyvi import ViTokenizer\n",
        "import re\n",
        "import html\n",
        "import contractions\n",
        "import requests\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42fLcaN_kPxf",
        "papermill": {
          "duration": 0.034616,
          "end_time": "2020-09-16T13:42:48.227940",
          "exception": false,
          "start_time": "2020-09-16T13:42:48.193324",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# 2. Data Preparation & Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-16T13:43:11.780740Z",
          "iopub.status.busy": "2020-09-16T13:43:11.779292Z",
          "iopub.status.idle": "2020-09-16T13:43:14.781059Z",
          "shell.execute_reply": "2020-09-16T13:43:14.779973Z"
        },
        "id": "N7Da1d8Pb-p4",
        "papermill": {
          "duration": 3.045516,
          "end_time": "2020-09-16T13:43:14.781209",
          "exception": false,
          "start_time": "2020-09-16T13:43:11.735693",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "# !python -m spacy download en_core_web_sm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "l8s-Fa8kXcOC"
      },
      "outputs": [],
      "source": [
        "class Data:\n",
        "  def __init__(self, url_en:str, url_vi:str):\n",
        "    Data.__check_dict = { # bổ xung\n",
        "      ' \\'s': '\\'s',\n",
        "      '& lt ;': '<',\n",
        "      '& gt ;': '>',\n",
        "      \"<[^<]+>\":'',\n",
        "      ' +': ' ',\n",
        "    }\n",
        "\n",
        "    data_en = requests.get(url_en).text.strip().splitlines()\n",
        "    data_vi = requests.get(url_vi).text.strip().splitlines()\n",
        "    self.__data_en = [self.__text_preprocessing(en, 'en') for en in data_en]\n",
        "    self.__data_vi = [self.__text_preprocessing(vi, 'vi') for vi in data_vi]\n",
        "\n",
        "  def __text_preprocessing(self, text: str, language: str = 'en'):\n",
        "    text = html.unescape(text)\n",
        "    for pattern, repl in Data.__check_dict.items():\n",
        "      text = text.lower()\n",
        "      text = re.sub(pattern, repl, text)\n",
        "\n",
        "    if language == 'en':\n",
        "      text = re.sub(' +', ' ', contractions.fix(text))\n",
        "\n",
        "    return text\n",
        "\n",
        "  @property\n",
        "  def en(self):\n",
        "    return self.__data_en\n",
        "\n",
        "  @property\n",
        "  def vi(self):\n",
        "    return self.__data_vi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-16T13:43:15.184744Z",
          "iopub.status.busy": "2020-09-16T13:43:15.183919Z",
          "iopub.status.idle": "2020-09-16T13:43:27.861623Z",
          "shell.execute_reply": "2020-09-16T13:43:27.860506Z"
        },
        "id": "yn8CDZ1ssIju",
        "papermill": {
          "duration": 12.722893,
          "end_time": "2020-09-16T13:43:27.861751",
          "exception": false,
          "start_time": "2020-09-16T13:43:15.138858",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "url = \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/\"\n",
        "\n",
        "train_data = Data(url +'train.en',url +'train.vi')\n",
        "val_data = Data(url + 'tst2012.en',url + 'tst2012.vi')\n",
        "test_data = Data(url + 'tst2013.en',url + 'tst2013.vi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM85G7nnJgmP",
        "outputId": "d536906d-df8e-4e60-f018-bbbfb8bc0e63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['khoa học đằng sau một tiêu đề về khí hậu',\n",
              " 'trong 4 phút , chuyên gia hoá học khí quyển rachel pike giới thiệu sơ lược về những nỗ lực khoa học miệt mài đằng sau những tiêu đề táo bạo về biến đổi khí hậu , cùng với đoàn nghiên cứu của mình -- hàng ngàn người đã cống hiến cho dự án này -- một chuyến bay mạo hiểm qua rừng già để tìm kiếm thông tin về một phân tử then chốt .',\n",
              " 'tôi muốn cho các bạn biết về sự to lớn của những nỗ lực khoa học đã góp phần làm nên các dòng tít bạn thường thấy trên báo .',\n",
              " 'có những dòng trông như thế này khi bàn về biến đổi khí hậu , và như thế này khi nói về chất lượng không khí hay khói bụi .',\n",
              " 'cả hai đều là một nhánh của cùng một lĩnh vực trong ngành khoa học khí quyển .']"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_data.vi[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-5tLn_BwdYk6"
      },
      "outputs": [],
      "source": [
        "class Language:\n",
        "  def __init__(self, data: Data, min_freq:int = 1):\n",
        "    specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "    Language.__tokenizer = {\n",
        "      'vi': lambda text: list(map(lambda word: re.sub('_', ' ', word), ViTokenizer.tokenize(text).split())),\n",
        "      'en': get_tokenizer('spacy', language='en_core_web_sm')\n",
        "    }\n",
        "    self.__vocab_en = build_vocab_from_iterator(self.__yield_tokens(data.en,'en'), min_freq, specials)\n",
        "    self.__vocab_en.set_default_index(0)\n",
        "\n",
        "    self.__vocab_vi = build_vocab_from_iterator(self.__yield_tokens(data.vi,'vi'), min_freq, specials)\n",
        "    self.__vocab_vi.set_default_index(0)\n",
        "  \n",
        "  def __yield_tokens(self, data:List[str] , language:str = 'en'):\n",
        "    for line in data:\n",
        "      yield Language.__tokenizer[language](line)\n",
        "\n",
        "  @property\n",
        "  def en(self):\n",
        "    return self.__vocab_en\n",
        "\n",
        "  @property\n",
        "  def vi(self):\n",
        "    return self.__vocab_vi\n",
        "  \n",
        "  def text_pipeline(self, data:List[str], language:str = 'en') -> List[str]:\n",
        "    if language == 'en':\n",
        "      return [vocab.en.lookup_tokens([2,*vocab.en.lookup_indices(Language.__tokenizer[language](line)),3]) for line in data]\n",
        "    if language == 'vi':\n",
        "      return [vocab.vi.lookup_tokens([2,*vocab.vi.lookup_indices(Language.__tokenizer[language](line)),3]) for line in data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "LhIym25YYbki"
      },
      "outputs": [],
      "source": [
        "vocab = Language(train_data,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2020-09-16T13:43:27.967832Z",
          "iopub.status.busy": "2020-09-16T13:43:27.966607Z",
          "iopub.status.idle": "2020-09-16T13:43:27.970755Z",
          "shell.execute_reply": "2020-09-16T13:43:27.970022Z"
        },
        "id": "ILXUMSRVLhb-",
        "outputId": "3e5b0553-ba8a-48ef-ef08-f9f2ac131ea2",
        "papermill": {
          "duration": 0.060387,
          "end_time": "2020-09-16T13:43:27.970875",
          "exception": false,
          "start_time": "2020-09-16T13:43:27.910488",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unique tokens in source (en) vocabulary: 21928\n",
            "Unique tokens in target (vi) vocabulary: 16085\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unique tokens in source (en) vocabulary: {len(vocab.en)}\")\n",
        "print(f\"Unique tokens in target (vi) vocabulary: {len(vocab.vi)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1H1az6qbTqH",
        "outputId": "cb0a2401-70be-4055-f484-3188fd9cf588"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vocab.en['<sos>']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-16T13:43:29.010670Z",
          "iopub.status.busy": "2020-09-16T13:43:29.009562Z",
          "iopub.status.idle": "2020-09-16T13:43:29.013898Z",
          "shell.execute_reply": "2020-09-16T13:43:29.013395Z"
        },
        "id": "9Gmz5adIwbwF",
        "papermill": {
          "duration": 0.430386,
          "end_time": "2020-09-16T13:43:29.014006",
          "exception": false,
          "start_time": "2020-09-16T13:43:28.583620",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "train_en_prep = vocab.text_pipeline(train_data.en,'en')\n",
        "train_vi_prep = vocab.text_pipeline(train_data.vi,'vi')\n",
        "test_en_prep = vocab.text_pipeline(test_data.en,'en')\n",
        "test_vi_prep = vocab.text_pipeline(test_data.vi,'vi')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_ZmS81XFB8w",
        "outputId": "4f269ab9-7c0a-4af6-cdd3-6be422a2c3ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train en: ['<sos>', 'rachel', 'pike', ':', 'the', 'science', 'behind', 'a', 'climate', 'headline', '<eos>']\n",
            "train vi: ['<sos>', 'khoa học', 'đằng', 'sau', 'một', 'tiêu đề', 'về', 'khí hậu', '<eos>']\n",
            "test en: ['<sos>', 'when', 'i', 'was', 'little', ',', 'i', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'i', 'grew', 'up', 'singing', 'a', 'song', 'called', '\"', 'nothing', 'to', 'envy', '.', '\"', '<eos>']\n",
            "train en: ['<sos>', 'in', '4', 'minutes', ',', 'atmospheric', 'chemist', 'rachel', 'pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.', '<eos>']\n",
            "train vi: ['<sos>', 'trong', '4', 'phút', ',', 'chuyên gia', 'hoá học', 'khí quyển', 'rachel', 'pike', 'giới thiệu', 'sơ lược', 'về', 'những', 'nỗ lực', 'khoa học', 'miệt mài', 'đằng', 'sau', 'những', 'tiêu đề', 'táo bạo', 'về', 'biến đổi', 'khí hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên cứu', 'của', 'mình', '-', '-', 'hàng', 'ngàn', 'người', 'đã', 'cống hiến', 'cho', 'dự án', 'này', '-', '-', 'một', 'chuyến', 'bay', 'mạo hiểm', 'qua', 'rừng già', 'để', 'tìm kiếm', 'thông tin', 'về', 'một', 'phân tử', 'then chốt', '.', '<eos>']\n",
            "test en: ['<sos>', 'and', 'i', 'was', 'very', 'proud', '.', '<eos>']\n",
            "train en: ['<sos>', 'i', 'would', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.', '<eos>']\n",
            "train vi: ['<sos>', 'tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to lớn', 'của', 'những', 'nỗ lực', 'khoa học', 'đã', 'góp phần', 'làm nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.', '<eos>']\n",
            "test en: ['<sos>', 'in', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'kim', 'il', '-', 'sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'america', ',', 'south', 'korea', ',', 'japan', 'are', 'the', 'enemies', '.', '<eos>']\n",
            "train en: ['<sos>', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.', '<eos>']\n",
            "train vi: ['<sos>', 'có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến đổi', 'khí hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất lượng', 'không khí', 'hay', 'khói', 'bụi', '.', '<eos>']\n",
            "test en: ['<sos>', 'although', 'i', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'i', 'thought', 'i', 'would', 'spend', 'my', 'entire', 'life', 'in', 'north', 'korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>']\n",
            "train en: ['<sos>', 'they', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.', '<eos>']\n",
            "train vi: ['<sos>', 'cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh vực', 'trong', 'ngành', 'khoa học', 'khí quyển', '.', '<eos>']\n",
            "test en: ['<sos>', 'when', 'i', 'was', 'seven', 'years', 'old', ',', 'i', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'i', 'thought', 'my', 'life', 'in', 'north', 'korea', 'was', 'normal', '.', '<eos>']\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(f'train en: {train_en_prep[i]}')\n",
        "  print(f'train vi: {train_vi_prep[i]}')\n",
        "  print(f'test en: {test_en_prep[i]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "hM6nVWprEztA"
      },
      "outputs": [],
      "source": [
        "train_data = list(zip(train_en_prep,train_vi_prep))\n",
        "train_data.sort(key = lambda x: (len(x[0]), len(x[1])))\n",
        "test_data = list(zip(test_en_prep, test_data.en, test_data.vi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csSSNGXEBaRY",
        "outputId": "36facb56-3d03-499a-9f5d-138d10676506"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', 'when', 'i', 'was', 'little', ',', 'i', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'i', 'grew', 'up', 'singing', 'a', 'song', 'called', '\"', 'nothing', 'to', 'envy', '.', '\"', '<eos>'], 'when i was little , i thought my country was the best on the planet , and i grew up singing a song called \" nothing to envy . \"', 'khi tôi còn nhỏ , tôi nghĩ rằng bắctriều tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài \" chúng ta chẳng có gì phải ghen tị . \"')\n",
            "(['<sos>', 'and', 'i', 'was', 'very', 'proud', '.', '<eos>'], 'and i was very proud .', 'tôi đã rất tự hào về đất nước tôi .')\n",
            "(['<sos>', 'in', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'kim', 'il', '-', 'sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'america', ',', 'south', 'korea', ',', 'japan', 'are', 'the', 'enemies', '.', '<eos>'], 'in school , we spent a lot of time studying the history of kim il-sung , but we never learned much about the outside world , except that america , south korea , japan are the enemies .', 'ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch kim ii- sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc hoa kỳ , hàn quốc và nhật bản là kẻ thù của chúng tôi .')\n",
            "(['<sos>', 'although', 'i', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'i', 'thought', 'i', 'would', 'spend', 'my', 'entire', 'life', 'in', 'north', 'korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>'], 'although i often wondered about the outside world , i thought i would spend my entire life in north korea , until everything suddenly changed .', 'mặc dù tôi đã từng tự hỏi không biết thế giới bên ngoài kia như thế nào , nhưng tôi vẫn nghĩ rằng mình sẽ sống cả cuộc đời ở bắctriều tiên , cho tới khi tất cả mọi thứ đột nhiên thay đổi .')\n",
            "(['<sos>', 'when', 'i', 'was', 'seven', 'years', 'old', ',', 'i', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'i', 'thought', 'my', 'life', 'in', 'north', 'korea', 'was', 'normal', '.', '<eos>'], 'when i was seven years old , i saw my first public execution , but i thought my life in north korea was normal .', 'khi tôi lên 7 , tôi chứng kiến cảnh người ta xử bắn công khai lần đầu tiên trong đời , nhưng tôi vẫn nghĩ cuộc sống của mình ở đây là hoàn toàn bình thường .')\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(train_data[i])\n",
        "\n",
        "for i in range(5):   \n",
        "  print(test_data[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "y_5jm46jx7Ab"
      },
      "outputs": [],
      "source": [
        "def make_batch(data, batchsize = 32):\n",
        "  bb = []\n",
        "  ben = []\n",
        "  bvi = []\n",
        "  for en, vi in data: \n",
        "    ben.append(en)\n",
        "    bvi.append(vi)\n",
        "    if len(ben) >= batchsize:\n",
        "      bb.append((ben, bvi))\n",
        "      ben = []\n",
        "      bvi = []\n",
        "  if len(ben) > 0:\n",
        "    bb.append((ben, bvi))\n",
        "  return bb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nsC2S7MTBcvY"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "train_data = make_batch(train_data, BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9ZIuqsvBewo",
        "outputId": "ca5c1680-6287-4d3c-8164-72454e95572d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']], [['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']])\n",
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']], [['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']])\n",
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']], [['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']])\n",
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']], [['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '̣', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '̣', '<eos>'], ['<sos>', 'cười', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '.', '<eos>']])\n",
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'ok', '.', '<eos>'], ['<sos>', 'okay', '.', '<eos>'], ['<sos>', 'useless', '.', '<eos>'], ['<sos>', 'why', '?', '<eos>'], ['<sos>', 'oh', '.', '<eos>'], ['<sos>', '<unk>', '.', '<eos>'], ['<sos>', 'indeed', '.', '<eos>']], [['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'cám ơn', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'khán giả', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '.', '<eos>'], ['<sos>', 'vỗ tay', '.', '<eos>'], ['<sos>', 'vỗ tay', '.', '<eos>'], ['<sos>', 'vỗ tay', '.', '<eos>'], ['<sos>', '\"', 'tán thưởng', '\"', '<eos>'], ['<sos>', 'có', 'những', 'giấc', 'mơ', 'đẹp', '.', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'ok', '<eos>'], ['<sos>', 'ok', '<eos>'], ['<sos>', 'vô dụng', '<eos>'], ['<sos>', 'tại sao', '<eos>'], ['<sos>', 'ồ', '<eos>'], ['<sos>', '<unk>', '<eos>'], ['<sos>', 'thực vậy', '<eos>']])\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "  print(train_data[i])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "9cBjckv0Bi2X"
      },
      "outputs": [],
      "source": [
        "def padding_batch(b):\n",
        "  maxlen = max([len(x) for x in b])\n",
        "  for tkl in b:\n",
        "    for i in range(maxlen - len(tkl)):\n",
        "      tkl.append('<pad>')\n",
        "\n",
        "def padding(bb):\n",
        "  for ben, bvi in bb:\n",
        "    padding_batch(ben)\n",
        "    padding_batch(bvi)\n",
        "\n",
        "padding(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "pqHcGS22HKHL"
      },
      "outputs": [],
      "source": [
        "train_data = [([vocab.en.lookup_indices(en) for en in ben],[vocab.vi.lookup_indices(vi)for vi in bvi]) for ben, bvi in train_data]\n",
        "test_data = [(vocab.en.lookup_indices(enprep), en, vi) for enprep, en, vi in test_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKfjpGMFS4Ye",
        "outputId": "661f5b15-11ed-4cae-e331-3befa705b663"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 57, 13, 25, 123, 4, 13, 182, 43, 252, 25, 6, 276, 29, 6, 475, 4, 7, 13, 995, 69, 1980, 11, 935, 158, 22, 361, 8, 6933, 5, 22, 3], 'when i was little , i thought my country was the best on the planet , and i grew up singing a song called \" nothing to envy . \"', 'khi tôi còn nhỏ , tôi nghĩ rằng bắctriều tiên là đất nước tốt nhất trên thế giới và tôi thường hát bài \" chúng ta chẳng có gì phải ghen tị . \"')\n",
            "([2, 7, 13, 25, 63, 1792, 5, 3], 'and i was very proud .', 'tôi đã rất tự hào về đất nước tôi .')\n",
            "([2, 14, 218, 4, 17, 600, 11, 137, 9, 90, 1452, 6, 409, 9, 7225, 11420, 33, 14016, 4, 28, 17, 187, 465, 115, 36, 6, 535, 85, 4, 1132, 12, 536, 4, 671, 3350, 4, 1578, 19, 6, 4683, 5, 3], 'in school , we spent a lot of time studying the history of kim il-sung , but we never learned much about the outside world , except that america , south korea , japan are the enemies .', 'ở trường , chúng tôi dành rất nhiều thời gian để học về cuộc đời của chủ tịch kim ii- sung , nhưng lại không học nhiều về thế giới bên ngoài , ngoại trừ việc hoa kỳ , hàn quốc và nhật bản là kẻ thù của chúng tôi .')\n"
          ]
        }
      ],
      "source": [
        "for i in range (3): \n",
        "  print(train_data[i]) \n",
        "for i in range (3): \n",
        "  print(test_data[i]) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "df-QJ3xagAXl"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"LSTM.model\"\n",
        "EPOCH = 10\n",
        "LR = 0.0001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "EoqN9u6Zf4iN"
      },
      "outputs": [],
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "class LSTM(torch.nn.Module):\n",
        "  def __init__(self, vocablist_x, vocabidx_x, vocablist_y, vocabidx_y):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.encemb = torch.nn.Embedding(len(vocablist_x), 256, padding_idx = vocabidx_x['<pad>'])\n",
        "    self.dropout = torch.nn.Dropout(0.5)\n",
        "    self.enclstm = torch.nn.LSTM(256,516,2,dropout=0.5)\n",
        "    \n",
        "    self.decemb = torch.nn.Embedding(len(vocablist_x), 256, padding_idx = vocabidx_y['<pad>'])\n",
        "    self.declstm = torch.nn.LSTM(256,516,2,dropout=0.5)\n",
        "    self.decout = torch.nn.Linear(516, len(vocabidx_y))\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x, y = x[0], x[1]\n",
        "    # print(x.size())\n",
        "    # print(y.size())\n",
        "\n",
        "    e_x = self.dropout(self.encemb(x))\n",
        "    \n",
        "    outenc,(hidden,cell) = self.enclstm(e_x)\n",
        "\n",
        "    n_y=y.shape[0]\n",
        "    loss = torch.tensor(0.,dtype=torch.float32).to(DEVICE)\n",
        "    for i in range(n_y-1):\n",
        "      input = y[i]\n",
        "      input = input.unsqueeze(0)\n",
        "      input = self.dropout(self.decemb(input))\n",
        "      outdec, (hidden,cell) = self.declstm(input,(hidden,cell))\n",
        "      output = self.decout(outdec.squeeze(0))\n",
        "      input = y[i+1]\n",
        "      loss += cross_entropy(output, y[i+1])\n",
        "    return loss\n",
        "\n",
        "  def evaluate(self,x,vocablist_y,vocabidx_y):\n",
        "    e_x = self.dropout(self.encemb(x))\n",
        "    outenc,(hidden,cell)=self.enclstm(e_x)\n",
        "    \n",
        "    y = torch.tensor([vocabidx_y['<cls>']]).to(DEVICE)\n",
        "    pred=[]\n",
        "    for i in range(30):\n",
        "      input = y\n",
        "      input = input.unsqueeze(0)\n",
        "      input = self.dropout(self.decemb(input))\n",
        "      outdec,(hidden,cell)= self.declstm(input,(hidden,cell))\n",
        "      output = self.decout(outdec.squeeze(0))  \n",
        "      pred_id = output.squeeze().argmax().item()\n",
        "      if pred_id == vocabidx_y['<eos>']:\n",
        "        break\n",
        "      pred_y = vocablist_y[pred_id][0]\n",
        "      pred.append(pred_y)\n",
        "      y[0]=pred_id\n",
        "      input=y\n",
        "    return pred  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J76wK2OUf7uG"
      },
      "outputs": [],
      "source": [
        "def train_LMST():\n",
        "  model = LSTM(vocab.en.get_itos(), vocab.en, vocab.vi.get_itos(), vocab.vi).to(DEVICE)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LR) \n",
        "  for epoch in range(EPOCH):\n",
        "    loss = 0\n",
        "    step = 0\n",
        "    for ben, bvi in train_data:\n",
        "      ben = torch.tensor(ben, dtype=torch.int64).transpose(0,1).to(DEVICE) \n",
        "      bvi = torch.tensor(bvi, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
        "      optimizer.zero_grad()\n",
        "      batchloss = model((ben, bvi))\n",
        "      batchloss.backward()\n",
        "      optimizer.step() \n",
        "      loss = loss + batchloss.item()\n",
        "      # if step % 100 == 0:\n",
        "      #   print(\"step:\", step, \"batch loss:\", batchloss.item())\n",
        "      # step += 1\n",
        "    print(\"epoch\", epoch, \": loss\", loss)\n",
        "  torch.save(model.state_dict(), MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTo7aMCng25A"
      },
      "outputs": [],
      "source": [
        "train_LMST()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITU7Z9z5iNX2"
      },
      "outputs": [],
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "\n",
        "def test_LMST():\n",
        "  model = LSTM(vocab.en.get_itos(), vocab.en, vocab.vi.get_itos(), vocab.vi).to(DEVICE)\n",
        "  model.load_state_dict(torch.load(MODEL_NAME))\n",
        "  model.eval()\n",
        "  ref = []\n",
        "  pred = []\n",
        "  for enprep, en, vi in test_data:\n",
        "    input = torch.tensor([enprep], dtype=torch.int64).transpose(0, 1).to(DEVICE)\n",
        "    p=model.evaluate(input, vocab.vi.get_itos(), vocab.vi)\n",
        "    print(\"INPUT\", en)\n",
        "    print(\"REF\", vi)\n",
        "    print(\"MT\", p)\n",
        "    ref.append([vi])\n",
        "    pred.append(p)\n",
        "  bleu = bleu_score(pred, ref)\n",
        "  print(\"total:\", len(test_data)) \n",
        "  print(\"bleu:\", bleu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wgLhVU-iOGO"
      },
      "outputs": [],
      "source": [
        "test_LMST()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "seq2seq-model-for-neural-machine-translation.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "papermill": {
      "duration": 6120.011978,
      "end_time": "2020-09-16T15:24:30.704300",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-09-16T13:42:30.692322",
      "version": "2.1.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "5ae58900cfbb8c43ab3495913814b7cf26024f51651a94ce8bf64d6111688e8d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
