{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSuJ-9X_qk1b",
        "papermill": {
          "duration": 0.035121,
          "end_time": "2020-09-16T13:42:35.566107",
          "exception": false,
          "start_time": "2020-09-16T13:42:35.530986",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Necessary Imports "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-16T13:42:35.644453Z",
          "iopub.status.busy": "2020-09-16T13:42:35.643680Z",
          "iopub.status.idle": "2020-09-16T13:42:48.157700Z",
          "shell.execute_reply": "2020-09-16T13:42:48.157103Z"
        },
        "id": "S-Ycz13hbUbC",
        "outputId": "85301260-538a-466a-815c-4c3f3d4d31e4",
        "papermill": {
          "duration": 12.557466,
          "end_time": "2020-09-16T13:42:48.157807",
          "exception": false,
          "start_time": "2020-09-16T13:42:35.600341",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.72-py2.py3-none-any.whl (8.3 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 40.1 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 73.8 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.72 pyahocorasick-1.4.4 textsearch-0.0.21\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 8.5 MB 25.1 MB/s \n",
            "\u001b[?25hCollecting sklearn-crfsuite\n",
            "  Downloading sklearn_crfsuite-0.3.6-py2.py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyvi) (1.0.2)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.21.6)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyvi) (1.1.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (4.64.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (1.15.0)\n",
            "Collecting python-crfsuite>=0.8.3\n",
            "  Downloading python_crfsuite-0.9.8-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (965 kB)\n",
            "\u001b[K     |████████████████████████████████| 965 kB 59.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from sklearn-crfsuite->pyvi) (0.8.9)\n",
            "Installing collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.8 pyvi-0.1.1 sklearn-crfsuite-0.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install contractions\n",
        "!pip install pyvi\n",
        "import torch\n",
        "from torch import nn, optim\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.metrics import bleu_score\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from pyvi import ViTokenizer\n",
        "import re\n",
        "import html\n",
        "import contractions\n",
        "import requests\n",
        "from typing import Iterator,List, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42fLcaN_kPxf",
        "papermill": {
          "duration": 0.034616,
          "end_time": "2020-09-16T13:42:48.227940",
          "exception": false,
          "start_time": "2020-09-16T13:42:48.193324",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# 2. Data Preparation & Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-16T13:43:11.780740Z",
          "iopub.status.busy": "2020-09-16T13:43:11.779292Z",
          "iopub.status.idle": "2020-09-16T13:43:14.781059Z",
          "shell.execute_reply": "2020-09-16T13:43:14.779973Z"
        },
        "id": "N7Da1d8Pb-p4",
        "papermill": {
          "duration": 3.045516,
          "end_time": "2020-09-16T13:43:14.781209",
          "exception": false,
          "start_time": "2020-09-16T13:43:11.735693",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cea31d16-cb56-43a3-a09c-6a44094daf89"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 12.8 MB 27.2 MB/s \n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!python -m spacy download en_core_web_sm --quiet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Data:\n",
        "  def __init__(self, url_en:str, url_vi:str):\n",
        "    Data.check_dict = { # bổ xung\n",
        "      ' \\'s': '\\'s',\n",
        "      '& lt ;': '<',\n",
        "      '& gt ;': '>',\n",
        "      \"<[^<]+>\":'',\n",
        "      ' +': ' ',\n",
        "    }\n",
        "    Data.tokenizer = {\n",
        "      'vi': lambda text: list(map(lambda word: re.sub('_', ' ', word), ViTokenizer.tokenize(text).split())),\n",
        "      'en': get_tokenizer('spacy', language='en_core_web_sm')\n",
        "    }\n",
        "    data_en = requests.get(url_en).text.strip().splitlines()\n",
        "    data_vi = requests.get(url_vi).text.strip().splitlines()\n",
        "    self.__data_en = [self.__text_preprocessing(en, 'en') for en in data_en]\n",
        "    self.__data_vi = [self.__text_preprocessing(vi, 'vi') for vi in data_vi]\n",
        "\n",
        "  def __text_preprocessing(self, text: str, language: str = 'en'):\n",
        "    text = html.unescape(text)\n",
        "    for pattern, repl in Data.check_dict.items():\n",
        "      text = text.lower()\n",
        "      text = re.sub(pattern, repl, text)\n",
        "\n",
        "    if language == 'en':\n",
        "      text = re.sub(' +', ' ', contractions.fix(text))\n",
        "      return self.tokenizer['en'](text)\n",
        "    return self.tokenizer['vi'](text)\n",
        "\n",
        "  @property\n",
        "  def en(self):\n",
        "    return self.__data_en\n",
        "\n",
        "  @property\n",
        "  def vi(self):\n",
        "    return self.__data_vi\n",
        "  \n"
      ],
      "metadata": {
        "id": "l8s-Fa8kXcOC"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-16T13:43:15.184744Z",
          "iopub.status.busy": "2020-09-16T13:43:15.183919Z",
          "iopub.status.idle": "2020-09-16T13:43:27.861623Z",
          "shell.execute_reply": "2020-09-16T13:43:27.860506Z"
        },
        "id": "yn8CDZ1ssIju",
        "papermill": {
          "duration": 12.722893,
          "end_time": "2020-09-16T13:43:27.861751",
          "exception": false,
          "start_time": "2020-09-16T13:43:15.138858",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "url = \"https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/\"\n",
        "\n",
        "train_data = Data(url +'train.en',url +'train.vi')\n",
        "# val_data = Data(url + 'tst2012.en',url + 'tst2012.vi')\n",
        "test_data = Data(url + 'tst2013.en',url + 'tst2013.vi')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for text in train_data.vi[:5]:\n",
        "  print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aM85G7nnJgmP",
        "outputId": "3f0d4e3e-d8a3-4870-e085-1017f99169ff"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['khoa học', 'đằng', 'sau', 'một', 'tiêu đề', 'về', 'khí hậu']\n",
            "['trong', '4', 'phút', ',', 'chuyên gia', 'hoá học', 'khí quyển', 'rachel', 'pike', 'giới thiệu', 'sơ lược', 'về', 'những', 'nỗ lực', 'khoa học', 'miệt mài', 'đằng', 'sau', 'những', 'tiêu đề', 'táo bạo', 'về', 'biến đổi', 'khí hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên cứu', 'của', 'mình', '-', '-', 'hàng', 'ngàn', 'người', 'đã', 'cống hiến', 'cho', 'dự án', 'này', '-', '-', 'một', 'chuyến', 'bay', 'mạo hiểm', 'qua', 'rừng già', 'để', 'tìm kiếm', 'thông tin', 'về', 'một', 'phân tử', 'then chốt', '.']\n",
            "['tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to lớn', 'của', 'những', 'nỗ lực', 'khoa học', 'đã', 'góp phần', 'làm nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.']\n",
            "['có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến đổi', 'khí hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất lượng', 'không khí', 'hay', 'khói', 'bụi', '.']\n",
            "['cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh vực', 'trong', 'ngành', 'khoa học', 'khí quyển', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Language:\n",
        "  def __init__(self, data: Data, min_freq:int = 1):\n",
        "    specials = [\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"]\n",
        "\n",
        "    self.__vocab_en = build_vocab_from_iterator(self.__yield_tokens(data.en,'en'), min_freq, specials)\n",
        "    self.__vocab_en.set_default_index(0)\n",
        "\n",
        "    self.__vocab_vi = build_vocab_from_iterator(self.__yield_tokens(data.vi,'vi'), min_freq, specials)\n",
        "    self.__vocab_vi.set_default_index(0)\n",
        "  \n",
        "  def __yield_tokens(self, data:List[str] , language:str = 'en'):\n",
        "    for line in data:\n",
        "      yield line\n",
        "\n",
        "  @property\n",
        "  def en(self):\n",
        "    return self.__vocab_en\n",
        "\n",
        "  @property\n",
        "  def vi(self):\n",
        "    return self.__vocab_vi\n",
        "\n",
        "  def text_pipeline(self, data:List[str], language:str = 'en') -> List[str]:\n",
        "    if language == 'en':\n",
        "      return [vocab.en.lookup_tokens([2,*vocab.en.lookup_indices(line),3]) for line in data]\n",
        "    if language == 'vi':\n",
        "      return [vocab.vi.lookup_tokens([2,*vocab.vi.lookup_indices(line),3]) for line in data]"
      ],
      "metadata": {
        "id": "-5tLn_BwdYk6"
      },
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Language(train_data,3)"
      ],
      "metadata": {
        "id": "LhIym25YYbki"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-16T13:43:27.967832Z",
          "iopub.status.busy": "2020-09-16T13:43:27.966607Z",
          "iopub.status.idle": "2020-09-16T13:43:27.970755Z",
          "shell.execute_reply": "2020-09-16T13:43:27.970022Z"
        },
        "id": "ILXUMSRVLhb-",
        "outputId": "58ee31a6-4ccf-48c5-ee47-0d16680a7d61",
        "papermill": {
          "duration": 0.060387,
          "end_time": "2020-09-16T13:43:27.970875",
          "exception": false,
          "start_time": "2020-09-16T13:43:27.910488",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique tokens in source (en) vocabulary: 21928\n",
            "Unique tokens in target (vi) vocabulary: 16085\n"
          ]
        }
      ],
      "source": [
        "print(f\"Unique tokens in source (en) vocabulary: {len(vocab.en)}\")\n",
        "print(f\"Unique tokens in target (vi) vocabulary: {len(vocab.vi)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2020-09-16T13:43:29.010670Z",
          "iopub.status.busy": "2020-09-16T13:43:29.009562Z",
          "iopub.status.idle": "2020-09-16T13:43:29.013898Z",
          "shell.execute_reply": "2020-09-16T13:43:29.013395Z"
        },
        "id": "9Gmz5adIwbwF",
        "papermill": {
          "duration": 0.430386,
          "end_time": "2020-09-16T13:43:29.014006",
          "exception": false,
          "start_time": "2020-09-16T13:43:28.583620",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "#vector\n",
        "train_en_prep = vocab.text_pipeline(train_data.en,'en')\n",
        "train_vi_prep = vocab.text_pipeline(train_data.vi,'vi')\n",
        "test_en_prep = vocab.text_pipeline(test_data.en,'en')\n",
        "test_vi_prep = vocab.text_pipeline(test_data.vi,'vi')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(f'train en: {train_en_prep[i]}')\n",
        "  print(f'train vi: {train_vi_prep[i]}')\n",
        "  print(f'test en: {test_en_prep[i]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_ZmS81XFB8w",
        "outputId": "5c8b43ef-394b-4b3a-f84a-119d7c77899d"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train en: ['<sos>', 'rachel', 'pike', ':', 'the', 'science', 'behind', 'a', 'climate', 'headline', '<eos>']\n",
            "train vi: ['<sos>', 'khoa học', 'đằng', 'sau', 'một', 'tiêu đề', 'về', 'khí hậu', '<eos>']\n",
            "test en: ['<sos>', 'when', 'i', 'was', 'little', ',', 'i', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'i', 'grew', 'up', 'singing', 'a', 'song', 'called', '\"', 'nothing', 'to', 'envy', '.', '\"', '<eos>']\n",
            "train en: ['<sos>', 'in', '4', 'minutes', ',', 'atmospheric', 'chemist', 'rachel', 'pike', 'provides', 'a', 'glimpse', 'of', 'the', 'massive', 'scientific', 'effort', 'behind', 'the', 'bold', 'headlines', 'on', 'climate', 'change', ',', 'with', 'her', 'team', '--', 'one', 'of', 'thousands', 'who', 'contributed', '--', 'taking', 'a', 'risky', 'flight', 'over', 'the', 'rainforest', 'in', 'pursuit', 'of', 'data', 'on', 'a', 'key', 'molecule', '.', '<eos>']\n",
            "train vi: ['<sos>', 'trong', '4', 'phút', ',', 'chuyên gia', 'hoá học', 'khí quyển', 'rachel', 'pike', 'giới thiệu', 'sơ lược', 'về', 'những', 'nỗ lực', 'khoa học', 'miệt mài', 'đằng', 'sau', 'những', 'tiêu đề', 'táo bạo', 'về', 'biến đổi', 'khí hậu', ',', 'cùng', 'với', 'đoàn', 'nghiên cứu', 'của', 'mình', '-', '-', 'hàng', 'ngàn', 'người', 'đã', 'cống hiến', 'cho', 'dự án', 'này', '-', '-', 'một', 'chuyến', 'bay', 'mạo hiểm', 'qua', 'rừng già', 'để', 'tìm kiếm', 'thông tin', 'về', 'một', 'phân tử', 'then chốt', '.', '<eos>']\n",
            "test en: ['<sos>', 'and', 'i', 'was', 'very', 'proud', '.', '<eos>']\n",
            "train en: ['<sos>', 'i', 'would', 'like', 'to', 'talk', 'to', 'you', 'today', 'about', 'the', 'scale', 'of', 'the', 'scientific', 'effort', 'that', 'goes', 'into', 'making', 'the', 'headlines', 'you', 'see', 'in', 'the', 'paper', '.', '<eos>']\n",
            "train vi: ['<sos>', 'tôi', 'muốn', 'cho', 'các', 'bạn', 'biết', 'về', 'sự', 'to lớn', 'của', 'những', 'nỗ lực', 'khoa học', 'đã', 'góp phần', 'làm nên', 'các', 'dòng', 'tít', 'bạn', 'thường', 'thấy', 'trên', 'báo', '.', '<eos>']\n",
            "test en: ['<sos>', 'in', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'kim', 'il', '-', 'sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'america', ',', 'south', 'korea', ',', 'japan', 'are', 'the', 'enemies', '.', '<eos>']\n",
            "train en: ['<sos>', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'climate', 'change', ',', 'and', 'headlines', 'that', 'look', 'like', 'this', 'when', 'they', 'have', 'to', 'do', 'with', 'air', 'quality', 'or', 'smog', '.', '<eos>']\n",
            "train vi: ['<sos>', 'có', 'những', 'dòng', 'trông', 'như', 'thế', 'này', 'khi', 'bàn', 'về', 'biến đổi', 'khí hậu', ',', 'và', 'như', 'thế', 'này', 'khi', 'nói', 'về', 'chất lượng', 'không khí', 'hay', 'khói', 'bụi', '.', '<eos>']\n",
            "test en: ['<sos>', 'although', 'i', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'i', 'thought', 'i', 'would', 'spend', 'my', 'entire', 'life', 'in', 'north', 'korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>']\n",
            "train en: ['<sos>', 'they', 'are', 'both', 'two', 'branches', 'of', 'the', 'same', 'field', 'of', 'atmospheric', 'science', '.', '<eos>']\n",
            "train vi: ['<sos>', 'cả', 'hai', 'đều', 'là', 'một', 'nhánh', 'của', 'cùng', 'một', 'lĩnh vực', 'trong', 'ngành', 'khoa học', 'khí quyển', '.', '<eos>']\n",
            "test en: ['<sos>', 'when', 'i', 'was', 'seven', 'years', 'old', ',', 'i', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'i', 'thought', 'my', 'life', 'in', 'north', 'korea', 'was', 'normal', '.', '<eos>']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = list(zip(train_en_prep,train_vi_prep))\n",
        "train_data.sort(key = lambda x: (len(x[0]), len(x[1])))\n",
        "test_data = list(zip(test_en_prep, test_data.en, test_data.vi))"
      ],
      "metadata": {
        "id": "hM6nVWprEztA"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(train_data[i])\n",
        "\n",
        "for i in range(5):   \n",
        "  print(test_data[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csSSNGXEBaRY",
        "outputId": "ee41017f-c1b2-4a5b-cb1f-d8afe1998e6d"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', '<eos>'], ['<sos>', '<eos>'])\n",
            "(['<sos>', 'when', 'i', 'was', 'little', ',', 'i', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'i', 'grew', 'up', 'singing', 'a', 'song', 'called', '\"', 'nothing', 'to', 'envy', '.', '\"', '<eos>'], ['when', 'i', 'was', 'little', ',', 'i', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'i', 'grew', 'up', 'singing', 'a', 'song', 'called', '\"', 'nothing', 'to', 'envy', '.', '\"'], ['khi', 'tôi', 'còn', 'nhỏ', ',', 'tôi', 'nghĩ', 'rằng', 'bắctriều', 'tiên', 'là', 'đất nước', 'tốt', 'nhất', 'trên', 'thế giới', 'và', 'tôi', 'thường', 'hát', 'bài', '\"', 'chúng ta', 'chẳng', 'có', 'gì', 'phải', 'ghen tị', '.', '\"'])\n",
            "(['<sos>', 'and', 'i', 'was', 'very', 'proud', '.', '<eos>'], ['and', 'i', 'was', 'very', 'proud', '.'], ['tôi', 'đã', 'rất', 'tự hào', 'về', 'đất nước', 'tôi', '.'])\n",
            "(['<sos>', 'in', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'kim', 'il', '-', 'sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'america', ',', 'south', 'korea', ',', 'japan', 'are', 'the', 'enemies', '.', '<eos>'], ['in', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'kim', 'il', '-', 'sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'america', ',', 'south', 'korea', ',', 'japan', 'are', 'the', 'enemies', '.'], ['ở', 'trường', ',', 'chúng tôi', 'dành', 'rất', 'nhiều', 'thời gian', 'để', 'học', 'về', 'cuộc đời', 'của', 'chủ tịch', 'kim', 'ii', '-', 'sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế giới', 'bên', 'ngoài', ',', 'ngoại trừ', 'việc', 'hoa', 'kỳ', ',', 'hàn quốc', 'và', 'nhật', 'bản', 'là', 'kẻ thù', 'của', 'chúng tôi', '.'])\n",
            "(['<sos>', 'although', 'i', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'i', 'thought', 'i', 'would', 'spend', 'my', 'entire', 'life', 'in', 'north', 'korea', ',', 'until', 'everything', 'suddenly', 'changed', '.', '<eos>'], ['although', 'i', 'often', 'wondered', 'about', 'the', 'outside', 'world', ',', 'i', 'thought', 'i', 'would', 'spend', 'my', 'entire', 'life', 'in', 'north', 'korea', ',', 'until', 'everything', 'suddenly', 'changed', '.'], ['mặc dù', 'tôi', 'đã', 'từng', 'tự', 'hỏi', 'không', 'biết', 'thế giới', 'bên', 'ngoài', 'kia', 'như', 'thế nào', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'rằng', 'mình', 'sẽ', 'sống', 'cả', 'cuộc đời', 'ở', 'bắctriều', 'tiên', ',', 'cho', 'tới', 'khi', 'tất cả', 'mọi', 'thứ', 'đột nhiên', 'thay đổi', '.'])\n",
            "(['<sos>', 'when', 'i', 'was', 'seven', 'years', 'old', ',', 'i', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'i', 'thought', 'my', 'life', 'in', 'north', 'korea', 'was', 'normal', '.', '<eos>'], ['when', 'i', 'was', 'seven', 'years', 'old', ',', 'i', 'saw', 'my', 'first', 'public', 'execution', ',', 'but', 'i', 'thought', 'my', 'life', 'in', 'north', 'korea', 'was', 'normal', '.'], ['khi', 'tôi', 'lên', '7', ',', 'tôi', 'chứng kiến', 'cảnh', 'người ta', 'xử', 'bắn', 'công khai', 'lần', 'đầu tiên', 'trong', 'đời', ',', 'nhưng', 'tôi', 'vẫn', 'nghĩ', 'cuộc sống', 'của', 'mình', 'ở', 'đây', 'là', 'hoàn toàn', 'bình thường', '.'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_batch(data, batchsize = 32):\n",
        "  bb = []\n",
        "  ben = []\n",
        "  bvi = []\n",
        "  for en, vi in data: \n",
        "    ben.append(en)\n",
        "    bvi.append(vi)\n",
        "    if len(ben) >= batchsize:\n",
        "      bb.append((ben, bvi))\n",
        "      ben = []\n",
        "      bvi = []\n",
        "  if len(ben) > 0:\n",
        "    bb.append((ben, bvi))\n",
        "  return bb\n"
      ],
      "metadata": {
        "id": "y_5jm46jx7Ab"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "train_data = make_batch(train_data, BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "nsC2S7MTBcvY"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  print(train_data[i])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9ZIuqsvBewo",
        "outputId": "1b0b5881-71f7-4796-c1fe-42eaa5f85354"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']], [['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']])\n",
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']], [['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']])\n",
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']], [['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']])\n",
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>']], [['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '̣', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '̣', '<eos>'], ['<sos>', 'cười', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', '.', '<eos>']])\n",
            "([['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'ok', '.', '<eos>'], ['<sos>', 'okay', '.', '<eos>'], ['<sos>', 'useless', '.', '<eos>'], ['<sos>', 'why', '?', '<eos>'], ['<sos>', 'oh', '.', '<eos>'], ['<sos>', '<unk>', '.', '<eos>'], ['<sos>', 'indeed', '.', '<eos>']], [['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '<eos>'], ['<sos>', 'cám ơn', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'khán giả', 'vỗ tay', '<eos>'], ['<sos>', 'vỗ tay', '.', '<eos>'], ['<sos>', 'vỗ tay', '.', '<eos>'], ['<sos>', 'vỗ tay', '.', '<eos>'], ['<sos>', 'vỗ tay', '.', '<eos>'], ['<sos>', '\"', 'tán thưởng', '\"', '<eos>'], ['<sos>', 'có', 'những', 'giấc', 'mơ', 'đẹp', '.', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', '.', '<eos>'], ['<sos>', 'ok', '<eos>'], ['<sos>', 'ok', '<eos>'], ['<sos>', 'vô dụng', '<eos>'], ['<sos>', 'tại sao', '<eos>'], ['<sos>', 'ồ', '<eos>'], ['<sos>', '<unk>', '<eos>'], ['<sos>', 'thực vậy', '<eos>']])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def padding_batch(b):\n",
        "  maxlen = max([len(x) for x in b])\n",
        "  for tkl in b:\n",
        "    for i in range(maxlen - len(tkl)):\n",
        "      tkl.append('<pad>')\n",
        "\n",
        "def padding(bb):\n",
        "  for ben, bvi in bb:\n",
        "    padding_batch(ben)\n",
        "    padding_batch(bvi)\n",
        "\n",
        "padding(train_data)"
      ],
      "metadata": {
        "id": "9cBjckv0Bi2X"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = [([vocab.en.lookup_indices(en) for en in ben],[vocab.vi.lookup_indices(vi)for vi in bvi]) for ben, bvi in train_data]\n",
        "test_data = [(vocab.en.lookup_indices(enprep), en, vi) for enprep, en, vi in test_data]"
      ],
      "metadata": {
        "id": "pqHcGS22HKHL"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range (3): \n",
        "  print(train_data[i]) \n",
        "for i in range (3): \n",
        "  print(test_data[i]) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKfjpGMFS4Ye",
        "outputId": "91a80797-39c2-4380-cb4e-24027103c850"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]], [[2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3], [2, 3]])\n",
            "([2, 57, 13, 25, 123, 4, 13, 182, 43, 252, 25, 6, 276, 29, 6, 475, 4, 7, 13, 995, 69, 1980, 11, 935, 158, 22, 361, 8, 6933, 5, 22, 3], ['when', 'i', 'was', 'little', ',', 'i', 'thought', 'my', 'country', 'was', 'the', 'best', 'on', 'the', 'planet', ',', 'and', 'i', 'grew', 'up', 'singing', 'a', 'song', 'called', '\"', 'nothing', 'to', 'envy', '.', '\"'], ['khi', 'tôi', 'còn', 'nhỏ', ',', 'tôi', 'nghĩ', 'rằng', 'bắctriều', 'tiên', 'là', 'đất nước', 'tốt', 'nhất', 'trên', 'thế giới', 'và', 'tôi', 'thường', 'hát', 'bài', '\"', 'chúng ta', 'chẳng', 'có', 'gì', 'phải', 'ghen tị', '.', '\"'])\n",
            "([2, 7, 13, 25, 63, 1792, 5, 3], ['and', 'i', 'was', 'very', 'proud', '.'], ['tôi', 'đã', 'rất', 'tự hào', 'về', 'đất nước', 'tôi', '.'])\n",
            "([2, 14, 218, 4, 17, 600, 11, 137, 9, 90, 1452, 6, 409, 9, 7225, 11420, 33, 14016, 4, 28, 17, 187, 465, 115, 36, 6, 535, 85, 4, 1132, 12, 536, 4, 671, 3350, 4, 1578, 19, 6, 4683, 5, 3], ['in', 'school', ',', 'we', 'spent', 'a', 'lot', 'of', 'time', 'studying', 'the', 'history', 'of', 'kim', 'il', '-', 'sung', ',', 'but', 'we', 'never', 'learned', 'much', 'about', 'the', 'outside', 'world', ',', 'except', 'that', 'america', ',', 'south', 'korea', ',', 'japan', 'are', 'the', 'enemies', '.'], ['ở', 'trường', ',', 'chúng tôi', 'dành', 'rất', 'nhiều', 'thời gian', 'để', 'học', 'về', 'cuộc đời', 'của', 'chủ tịch', 'kim', 'ii', '-', 'sung', ',', 'nhưng', 'lại', 'không', 'học', 'nhiều', 'về', 'thế giới', 'bên', 'ngoài', ',', 'ngoại trừ', 'việc', 'hoa', 'kỳ', ',', 'hàn quốc', 'và', 'nhật', 'bản', 'là', 'kẻ thù', 'của', 'chúng tôi', '.'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = \"LSTM.model\"\n",
        "EPOCH = 10\n",
        "LR = 0.0001\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "df-QJ3xagAXl"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "class LSTM(torch.nn.Module):\n",
        "  def __init__(self, vocablist_x, vocabidx_x, vocablist_y, vocabidx_y):\n",
        "    super(LSTM, self).__init__()\n",
        "\n",
        "    self.encemb = torch.nn.Embedding(len(vocablist_x), 256, padding_idx = vocabidx_x['<pad>'])\n",
        "    self.dropout = torch.nn.Dropout(0.5)\n",
        "    self.enclstm = torch.nn.LSTM(256,516,2,dropout=0.5)\n",
        "    \n",
        "    self.decemb = torch.nn.Embedding(len(vocablist_x), 256, padding_idx = vocabidx_y['<pad>'])\n",
        "    self.declstm = torch.nn.LSTM(256,516,2,dropout=0.5)\n",
        "    self.decout = torch.nn.Linear(516, len(vocabidx_y))\n",
        "  \n",
        "  def forward(self,x):\n",
        "    x, y = x[0], x[1]\n",
        "    # print(x.size())\n",
        "    # print(y.size())\n",
        "\n",
        "    e_x = self.dropout(self.encemb(x))\n",
        "    \n",
        "    outenc,(hidden,cell) = self.enclstm(e_x)\n",
        "\n",
        "    n_y=y.shape[0]\n",
        "    loss = torch.tensor(0.,dtype=torch.float32).to(DEVICE)\n",
        "    for i in range(n_y-1):\n",
        "      input = y[i]\n",
        "      input = input.unsqueeze(0)\n",
        "      input = self.dropout(self.decemb(input))\n",
        "      outdec, (hidden,cell) = self.declstm(input,(hidden,cell))\n",
        "      output = self.decout(outdec.squeeze(0))\n",
        "      input = y[i+1]\n",
        "      loss += cross_entropy(output, y[i+1])\n",
        "    return loss\n",
        "\n",
        "  def evaluate(self,x,vocablist_y,vocabidx_y):\n",
        "    e_x = self.dropout(self.encemb(x))\n",
        "    outenc,(hidden,cell)=self.enclstm(e_x)\n",
        "    \n",
        "    y = torch.tensor([vocabidx_y['<cls>']]).to(DEVICE)\n",
        "    pred=[]\n",
        "    for i in range(30):\n",
        "      input = y\n",
        "      input = input.unsqueeze(0)\n",
        "      input = self.dropout(self.decemb(input))\n",
        "      outdec,(hidden,cell)= self.declstm(input,(hidden,cell))\n",
        "      output = self.decout(outdec.squeeze(0))  \n",
        "      pred_id = output.squeeze().argmax().item()\n",
        "      if pred_id == vocabidx_y['<eos>']:\n",
        "        break\n",
        "      pred_y = vocablist_y[pred_id]\n",
        "      pred.append(pred_y)\n",
        "      y[0]=pred_id\n",
        "      input=y\n",
        "    return pred  "
      ],
      "metadata": {
        "id": "EoqN9u6Zf4iN"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_LMST():\n",
        "  model = LSTM(vocab.en.get_itos(), vocab.en, vocab.vi.get_itos(), vocab.vi).to(DEVICE)\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=LR) \n",
        "  for epoch in range(EPOCH):\n",
        "    loss = 0\n",
        "    step = 0\n",
        "    for ben, bvi in train_data:\n",
        "      ben = torch.tensor(ben, dtype=torch.int64).transpose(0,1).to(DEVICE) \n",
        "      bvi = torch.tensor(bvi, dtype=torch.int64).transpose(0,1).to(DEVICE)\n",
        "      optimizer.zero_grad()\n",
        "      batchloss = model((ben, bvi))\n",
        "      batchloss.backward()\n",
        "      optimizer.step() \n",
        "      loss = loss + batchloss.item()\n",
        "      # if step % 100 == 0:\n",
        "      #   print(\"step:\", step, \"batch loss:\", batchloss.item())\n",
        "      # step += 1\n",
        "    print(\"epoch\", epoch, \": loss\", loss)\n",
        "  torch.save(model.state_dict(), MODEL_NAME)"
      ],
      "metadata": {
        "id": "J76wK2OUf7uG"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_LMST()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTo7aMCng25A",
        "outputId": "b30794ef-533b-4619-eb33-0a9eea38b9fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch 0 : loss 525552.2081832886\n",
            "epoch 1 : loss 487123.3473596573\n",
            "epoch 2 : loss 462455.4591603279\n",
            "epoch 3 : loss 445120.3843970299\n",
            "epoch 4 : loss 431497.23986268044\n",
            "epoch 5 : loss 420206.5520154834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchtext.data.metrics import bleu_score\n",
        "def test_LMST():\n",
        "  model = LSTM(vocab.en.get_itos(), vocab.en, vocab.vi.get_itos(), vocab.vi).to(DEVICE)\n",
        "  model.load_state_dict(torch.load(MODEL_NAME))\n",
        "  model.eval()\n",
        "  ref = []\n",
        "  pred = []\n",
        "  for enprep, en, vi in test_data:\n",
        "    input = torch.tensor([enprep], dtype=torch.int64).transpose(0, 1).to(DEVICE)\n",
        "    p=model.evaluate(input, vocab.vi.get_itos(), vocab.vi)\n",
        "    print(\"INPUT\", en)\n",
        "    print(\"REF\", vi)\n",
        "    print(\"MT\", p)\n",
        "    ref.append([vi])\n",
        "    pred.append(p)\n",
        "  print(\"total:\", len(test_data)) \n",
        "  bleu = bleu_score(pred, ref,0,[1])\n",
        "  print(\"bleu:\", bleu)"
      ],
      "metadata": {
        "id": "ITU7Z9z5iNX2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_LMST()\n"
      ],
      "metadata": {
        "id": "7wgLhVU-iOGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "papermill": {
      "duration": 6120.011978,
      "end_time": "2020-09-16T15:24:30.704300",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2020-09-16T13:42:30.692322",
      "version": "2.1.0"
    },
    "colab": {
      "name": "seq2seq-model-for-neural-machine-translation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}